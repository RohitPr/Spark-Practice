{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"spark\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from CSV\n",
    "# df = spark.read.csv(\"raw.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply transformations similar to the provided SQL logic\n",
    "# result_df = df.filter(\n",
    "#     (col(\"data_header\") == \"Plant Run\") &\n",
    "#     (col(\"data_type\") == \"Association\") &\n",
    "#     (col(\"entity_type\") == \"SEED TRAIN LOT\") &\n",
    "#     (col(\"active\") == 1) &\n",
    "#     (col(\"delete_flag\") == 0)\n",
    "# ).select(\n",
    "#     col(\"string_data\").alias(\"plantrun_barcode\"),\n",
    "#     col(\"barcode\").alias(\"harvest_lot_barcode\"),\n",
    "#     lit(\"PLANT RUN -> SEED LOT\").alias(\"connection_type\")\n",
    "# )\n",
    "\n",
    "# result_df = result_df.union(\n",
    "#     df.filter(\n",
    "#         (col(\"data_header\") == \"Seed Train Or Production Sample Lot\") &\n",
    "#         (col(\"data_type\") == \"Association\") &\n",
    "#         (col(\"entity_type\") == \"PLANT RUN\") &\n",
    "#         (col(\"casted_data_type\") == \"SEED TRAIN LOT\") &\n",
    "#         (col(\"active\") == 1) &\n",
    "#         (col(\"delete_flag\") == 0)\n",
    "#     ).select(\n",
    "#         col(\"string_data\").alias(\"plantrun_barcode\"),\n",
    "#         col(\"barcode\").alias(\"harvest_lot_barcode\"),\n",
    "#         lit(\"SEED LOT -> PLANT RUN\").alias(\"connection_type\")\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# result_df = result_df.union(\n",
    "#     df.filter(\n",
    "#         (col(\"data_header\") == \"Seed Train Lot\") &\n",
    "#         (col(\"data_type\") == \"Association\") &\n",
    "#         (col(\"entity_type\") == \"PRODUCTION LOT\") &\n",
    "#         (col(\"active\") == 1) &\n",
    "#         (col(\"delete_flag\") == 0)\n",
    "#     ).select(\n",
    "#         col(\"string_data\").alias(\"plantrun_barcode\"),\n",
    "#         col(\"barcode\").alias(\"harvest_lot_barcode\"),\n",
    "#         lit(\"SEED LOT -> PRODUCTION LOT\").alias(\"connection_type\")\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = spark.read.csv(\"result.csv\", header=True)\n",
    "\n",
    "result_df = result_df.withColumnRenamed(\"HARVESTLOTBARCODE\", \"harvest_lot_barcode\") \\\n",
    "                     .withColumnRenamed(\"PLANTRUNBARCODE\", \"plantrun_barcode\")\n",
    "\n",
    "pd_subquery = result_df.filter(result_df[\"harvest_lot_barcode\"].startswith(\"PD\")).select(\n",
    "    col(\"plantrun_barcode\").alias(\"pd_plantrun_barcode\"),\n",
    "    col(\"harvest_lot_barcode\").alias(\"pd_lot_barcode\")\n",
    ")\n",
    "\n",
    "# Perform left join with the main DataFrame\n",
    "final_df = result_df.join(pd_subquery, pd_subquery.pd_plantrun_barcode == result_df.harvest_lot_barcode, \"left_outer\").select(\n",
    "    result_df[\"*\"],\n",
    "    pd_subquery[\"pd_lot_barcode\"],\n",
    "    split(result_df[\"harvest_lot_barcode\"], \"-\")[0].alias(\"hv_lot\"))\n",
    "\n",
    "cross_join_condition = col(\"f1.hv_lot\") == col(\"f2.hv_lot\")\n",
    "\n",
    "# Define cross join output\n",
    "cross_join_output = final_df.alias(\"f1\").crossJoin(\n",
    "    final_df.select(\"plantrun_barcode\", \"hv_lot\").distinct().alias(\"f2\")\n",
    ").filter(cross_join_condition).select(\n",
    "    col(\"f1.plantrun_barcode\"),\n",
    "    col(\"f1.harvest_lot_barcode\"),\n",
    "    col(\"f1.pd_lot_barcode\"),\n",
    "    col(\"f1.hv_lot\"),\n",
    "    col(\"f2.plantrun_barcode\").alias(\"passed_plantrun\")\n",
    ")\n",
    "\n",
    "cross_join_output_filtered = cross_join_output.filter(col(\"pd_lot_barcode\").isNotNull()).select(\n",
    "    col(\"plantrun_barcode\").alias(\"last_plantrun\"),\n",
    "    col(\"harvest_lot_barcode\").alias(\"last_seed_train_lot\"),\n",
    "    col(\"hv_lot\").alias(\"filter_hv_lot\")\n",
    ").distinct()\n",
    "\n",
    "pd_join_output = cross_join_output.join(\n",
    "    cross_join_output_filtered, cross_join_output.hv_lot == cross_join_output_filtered.filter_hv_lot, how = 'left').select(\n",
    "    cross_join_output.passed_plantrun,\n",
    "    cross_join_output_filtered.last_seed_train_lot,\n",
    "    cross_join_output_filtered.last_plantrun,\n",
    "    cross_join_output.plantrun_barcode,\n",
    "    cross_join_output.harvest_lot_barcode,\n",
    "    cross_join_output.hv_lot\n",
    ").withColumn(\"passed_plantrun_type\", lit('SEED TRAIN').cast(\"string\")) \\\n",
    " .withColumn(\"plantrun_type\", lit('SEED TRAIN').cast(\"string\"))\n",
    "\n",
    "window_spec = Window.orderBy(\"passed_plantrun\", \"last_seed_train_lot\", \"last_plantrun\", \"plantrun_barcode\", \"harvest_lot_barcode\", \"hv_lot\")\n",
    "\n",
    "# Add the 'novaflex_data_key' column using row_number() window function\n",
    "pd_join_output = pd_join_output.withColumn(\"novaflex_data_key\", row_number().over(window_spec))\n",
    "\n",
    "lineage_df = pd_join_output.select(col(\"passed_plantrun\"), col(\"plantrun_barcode\")).distinct()\n",
    "\n",
    "window_spec = Window.orderBy(\"passed_plantrun\", \"plantrun_barcode\")\n",
    "\n",
    "# Add the 'novaflex_data_key' column using row_number() window function\n",
    "lineage_df = lineage_df.withColumn(\"plant_datakey\", row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to mark rows where passed_plantrun = plantrun_barcode\n",
    "lineage_rank_df = lineage_df.withColumn(\"lineage_rank\",\n",
    "                           when(col(\"passed_plantrun\") == col(\"plantrun_barcode\"), 1)\n",
    "                           .otherwise(0))\n",
    "\n",
    "#  Odd Rank Logic\n",
    "\n",
    "def apply_logic_odd(df):\n",
    "    \n",
    "    #Window specification\n",
    "    window_spec = Window.partitionBy(\"passed_plantrun\").orderBy(\"plant_datakey\")\n",
    "    \n",
    "    # Apply the logic to all rows in the partition\n",
    "    updated_df = df.withColumn(\"lineage_rank\",\n",
    "                               when(lag(\"lineage_rank\").over(window_spec) >= 1,\n",
    "                                    lag(\"lineage_rank\").over(window_spec) + 2)\n",
    "                               .otherwise(col(\"lineage_rank\")))\n",
    "    \n",
    "    # Check for convergence\n",
    "    if updated_df.subtract(df).isEmpty():\n",
    "        return updated_df  # Convergence reached\n",
    "    else:\n",
    "        return apply_logic_odd(updated_df)  # Recursively apply logic\n",
    "\n",
    "# Apply the recursive function to the initial DataFrame\n",
    "odd_rank_df = apply_logic_odd(lineage_rank_df)\n",
    "\n",
    "odd_rank_df_final = odd_rank_df.filter(col(\"lineage_rank\")>= 1).withColumn('lineage_entity', lit('Downstream'))\n",
    "\n",
    "# Even Rank Logic\n",
    "\n",
    "# Define a recursive function to apply the logic until convergence\n",
    "def apply_logic_even(df):\n",
    "    # Define a window specification ordered by some column in descending order\n",
    "    window_spec = Window.partitionBy(\"passed_plantrun\").orderBy(col(\"plant_datakey\").desc())\n",
    "\n",
    "    # Apply the logic to all rows in the partition\n",
    "    updated_df = df.withColumn(\"lineage_rank\",\n",
    "                               when(lag(\"lineage_rank\").over(window_spec) == 1,\n",
    "                                    lag(\"lineage_rank\").over(window_spec) + 1)\n",
    "                               .when(lag(\"lineage_rank\").over(window_spec) > 1,\n",
    "                                    lag(\"lineage_rank\").over(window_spec) + 2)\n",
    "                               .otherwise(col(\"lineage_rank\")))\n",
    "    \n",
    "    # Check for convergence\n",
    "    if updated_df.subtract(df).isEmpty():\n",
    "        return updated_df  # Convergence reached\n",
    "    else:\n",
    "        return apply_logic_even(updated_df)  # Recursively apply logic\n",
    "\n",
    "even_rank_df = odd_rank_df.filter(col(\"lineage_rank\")<= 1)\n",
    "\n",
    "even_rank_df_final = apply_logic_even(even_rank_df)\n",
    "\n",
    "even_rank_df_final = even_rank_df_final.filter(col('lineage_rank') > 1).withColumn('lineage_entity', lit('Upstream'))\n",
    "\n",
    "lineage_final_rank_df = odd_rank_df_final.union(even_rank_df_final).dropDuplicates().orderBy(col('passed_plantrun'), col('plant_datakey'))\n",
    "\n",
    "final_output_df = pd_join_output.join(lineage_final_rank_df, (pd_join_output.passed_plantrun == lineage_final_rank_df.passed_plantrun) & (pd_join_output.plantrun_barcode == lineage_final_rank_df.plantrun_barcode), how='left').select(pd_join_output['*'],lineage_final_rank_df['lineage_rank'], lineage_final_rank_df['lineage_entity'])\n",
    "\n",
    "pandas_df = final_output_df.toPandas()\n",
    "\n",
    "# Write Pandas DataFrame to CSV\n",
    "pandas_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
